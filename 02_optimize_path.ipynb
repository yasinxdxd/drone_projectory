{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1222b879",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "**We will deeply discuss how we define the cost function and implement solvers through the code**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4311ec27",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "72fa44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5b1d02",
   "metadata": {},
   "source": [
    "## Defining the Cost Function\n",
    "<div style=\"font-size: 130%;\">\n",
    "\n",
    "We therefore define our cost function as the sum of two main components:\n",
    "a **position-related** cost and a **direction-related** cost.\n",
    "</div>\n",
    "\n",
    "$\n",
    "\\Large\n",
    "J(\\mathbf{p}, \\mathbf{n})\n",
    "\\;=\\;\n",
    "J^{\\text{pos}}(\\mathbf{p})\n",
    "\\;+\\;\n",
    "J^{\\text{dir}}(\\mathbf{n})\n",
    "$\n",
    "\n",
    "<div style=\"font-size: 130%;\">\n",
    "\n",
    "Since we assume that ground truth trajectory data is unknow, balance between data fidelity and smoothness is very important. \\\n",
    "So we can define our two parts of the **cost** function as also two parts:\n",
    "</div>\n",
    "\n",
    "$\n",
    "\\Large\n",
    "J^{\\text{pos}}(\\mathbf{p}) \\;=\\; J^{\\text{pos}}_{fidelety} + J^{\\text{pos}}_{smoothness}\n",
    "$\\\n",
    "$\n",
    "\\Large\n",
    "J^{\\text{dir}}(\\mathbf{n}) \\;=\\; J^{\\text{dir}}_{fidelety} + J^{\\text{dir}}_{smoothness}\n",
    "$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"font-size: 130%;\">\n",
    "\n",
    "Fidelity parts are relatively more intuative to figure out. \\\n",
    "Since we want our cost function to penalty our predictions when it is not close to the original (noisy) data, we can directly use `Sum Square Error` or `Mean Square Error`\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "$\n",
    "\\Large\n",
    "J^{\\text{pos}}_{fidelety}(\\mathbf{p}) \\;=\\;\n",
    "\\sum_{i=1}^{N} \\|\\mathbf{p}_i - \\mathbf{y^p}_i\\|^2\n",
    "$\\\n",
    "$\n",
    "\\Large\n",
    "J^{\\text{dir}}_{fidelety}(\\mathbf{n}) \\;=\\;\n",
    "\\sum_{i=1}^{N} \\|\\mathbf{n}_i - \\mathbf{y^n}_i\\|^2\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<div style=\"font-size: 130%;\">\n",
    "\n",
    "To **penalize non-smoothness** in the reconstructed trajectory,  \n",
    "we can examine the changes in position over discrete time steps.\n",
    "\n",
    "The **difference** between two consecutive positions approximates the **velocity**: \\\n",
    "$\n",
    "\\Large\n",
    "\\mathbf{v}_i = \\mathbf{p}_{i+1} - \\mathbf{p}_i\n",
    "$\n",
    "\n",
    "Similarly, the **change** in velocity between two steps approximates the **acceleration**: \\\n",
    "$\n",
    "\\Large\n",
    "\\mathbf{a}_i = \\mathbf{v}_i - \\mathbf{v}_{i-1}\n",
    "$\n",
    "\n",
    "Substituting the expressions for velocity, we get: \\\n",
    "$\n",
    "\\Large\n",
    "\\mathbf{a}_i \n",
    "= (\\mathbf{p}_{i+1} - \\mathbf{p}_i) - (\\mathbf{p}_i - \\mathbf{p}_{i-1})\n",
    "= \\mathbf{p}_{i+1} - 2\\mathbf{p}_i + \\mathbf{p}_{i-1}\n",
    "\\;\\approx\\; \\text{acceleration (rate of change of velocity)}.\n",
    "$\n",
    "\\\n",
    "\n",
    "Therefore, to **encourage smoothness**, we penalize large accelerations using the following regularization term: \\\n",
    "\\\n",
    "$\n",
    "\\Large\n",
    "J^{\\text{pos}}_{smoothness}(\\mathbf{p}) \\;=\\;\n",
    "\\lambda_{1} \\sum_{i=2}^{N-1} \\|\\mathbf{p}_{i+1} - 2\\mathbf{p}_i + \\mathbf{p}_{i-1}\\|^2\n",
    "$\\\n",
    "\\\n",
    "\\\n",
    "Exactly the same for direction, to **encourage smoothness**, \\\n",
    "\\\n",
    "$\n",
    "\\Large\n",
    "J^{\\text{dir}}_{smoothness}(\\mathbf{n}) \\;=\\;\n",
    "\\lambda_{2} \\sum_{i=2}^{N-1} \\|\\mathbf{n}_{i+1} - 2\\mathbf{n}_i + \\mathbf{n}_{i-1}\\|^2\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "So here our **cost** function: \\\n",
    "$\n",
    "\\Large\n",
    "J(\\mathbf{p}, \\mathbf{n}) \\;=\\;\n",
    "\\\\\n",
    "\\sum_{i=1}^{N} \\|\\mathbf{p}_i - \\mathbf{y^p}_i\\|^2\n",
    "\\;+\\;\n",
    "\\\\\n",
    "\\lambda_{1} \\sum_{i=2}^{N-1} \\|\\mathbf{p}_{i+1} - 2\\mathbf{p}_i + \\mathbf{p}_{i-1}\\|^2 \n",
    "\\;+\\;\n",
    "\\\\\n",
    "\\sum_{i=1}^{N} \\|\\mathbf{n}_i - \\mathbf{y^n}_i\\|^2\n",
    "\\;+\\;\n",
    "\\\\\n",
    "\\lambda_{2} \\sum_{i=2}^{N-1} \\|\\mathbf{n}_{i+1} - 2\\mathbf{n}_i + \\mathbf{n}_{i-1}\\|^2 \n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83dbe67",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c82769",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('drone_trajectory.npz')\n",
    "trajectory = data.get('trajectory_noisy')\n",
    "\n",
    "positions = trajectory[:, :3]  # y^p\n",
    "nv        = trajectory[:, 3:6] # y^n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe60e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cost(p, n, y_p, y_n, lam_1, lam_2):\n",
    "    # position fidelity\n",
    "    fidelity_p = np.sum(np.linalg.norm(p - y_p, axis=1)**2)\n",
    "\n",
    "    # position smoothness (second difference)\n",
    "    smooth_p = np.sum(\n",
    "        np.linalg.norm(p[2:] - 2*p[1:-1] + p[:-2], axis=1)**2\n",
    "    )\n",
    "\n",
    "    # direction fidelity\n",
    "    fidelity_n = np.sum(np.linalg.norm(n - y_n, axis=1)**2)\n",
    "\n",
    "    # direction smoothness\n",
    "    smooth_n = np.sum(\n",
    "        np.linalg.norm(n[2:] - 2*n[1:-1] + n[:-2], axis=1)**2\n",
    "    )\n",
    "\n",
    "    return fidelity_p + lam_1 * smooth_p + fidelity_n + lam_2 * smooth_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2560c0",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 130%;\">\n",
    "\n",
    "## Gradient of the Cost Function\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\nabla J(\\mathbf{p}, \\mathbf{n})\n",
    "\\;=\\;\n",
    "\\begin{bmatrix} \\frac{\\partial J(\\mathbf{p}, \\mathbf{n})}{\\partial \\mathbf{p}} \\\\ \\frac{\\partial J(\\mathbf{p}, \\mathbf{n})}{\\partial \\mathbf{n}} \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\frac{\\partial J(\\mathbf{p}, \\mathbf{n})}{\\partial \\mathbf{p}}\n",
    "\\;=\\;\n",
    "\\frac{\\partial J^{\\text{pos}}_{fidelety}(\\mathbf{p})}{\\partial \\mathbf{p}}\n",
    "\\;+\\;\n",
    "\\frac{\\partial J^{\\text{dir}}_{fidelety}(\\mathbf{n})}{\\partial \\mathbf{p}}\n",
    "\\;+\\;\n",
    "\\frac{\\partial J^{\\text{pos}}_{smoothness}(\\mathbf{p})}{\\partial \\mathbf{p}}\n",
    "\\;+\\;\n",
    "\\frac{\\partial J^{\\text{dir}}_{smoothness}(\\mathbf{n})}{\\partial \\mathbf{p}}\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\frac{\\partial J(\\mathbf{p}, \\mathbf{n})}{\\partial \\mathbf{n}} \n",
    "\\;=\\;\n",
    "\\frac{\\partial J^{\\text{pos}}_{fidelety}(\\mathbf{p})}{\\partial \\mathbf{n}}\n",
    "\\;+\\;\n",
    "\\frac{\\partial J^{\\text{dir}}_{fidelety}(\\mathbf{n})}{\\partial \\mathbf{n}}\n",
    "\\;+\\;\n",
    "\\frac{\\partial J^{\\text{pos}}_{smoothness}(\\mathbf{p})}{\\partial \\mathbf{n}}\n",
    "\\;+\\;\n",
    "\\frac{\\partial J^{\\text{dir}}_{smoothness}(\\mathbf{n})}{\\partial \\mathbf{n}}\n",
    "$\n",
    "\n",
    "Some of them **equals zero** when we get the derivative.\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\frac{\\partial J(\\mathbf{p}, \\mathbf{n})}{\\partial \\mathbf{p}}\n",
    "\\;=\\;\n",
    "\\frac{\\partial J^{\\text{pos}}_{fidelety}(\\mathbf{p})}{\\partial \\mathbf{p}}\n",
    "\\;+\\;\n",
    "\\frac{\\partial J^{\\text{pos}}_{smoothness}(\\mathbf{p})}{\\partial \\mathbf{p}}\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\frac{\\partial J(\\mathbf{p}, \\mathbf{n})}{\\partial \\mathbf{n}} \n",
    "\\;=\\;\n",
    "\\frac{\\partial J^{\\text{dir}}_{fidelety}(\\mathbf{n})}{\\partial \\mathbf{n}}\n",
    "\\;+\\;\n",
    "\\frac{\\partial J^{\\text{dir}}_{smoothness}(\\mathbf{n})}{\\partial \\mathbf{n}}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient of Position Fidelity Term\n",
    "\n",
    "$\n",
    "\\Large\n",
    "J^{\\text{pos}}_{\\text{fidelity}}(\\mathbf{p}) = \\sum_{i=1}^{N} \\|\\mathbf{p}_i - \\mathbf{y}^p_i\\|^2\n",
    "$\n",
    "\n",
    "Taking the derivative with respect to $\\mathbf{p}_i$:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\frac{\\partial J^{\\text{pos}}_{\\text{fidelity}}}{\\partial \\mathbf{p}_i} = 2(\\mathbf{p}_i - \\mathbf{y}^p_i)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient of Position Smoothness Term\n",
    "\n",
    "$\n",
    "\\Large\n",
    "J^{\\text{pos}}_{\\text{smoothness}}(\\mathbf{p}) = \\lambda_1 \\sum_{i=2}^{N-1} \\|\\mathbf{p}_{i+1} - 2\\mathbf{p}_i + \\mathbf{p}_{i-1}\\|^2\n",
    "$\n",
    "\n",
    "For each $\\mathbf{p}_i$, we need to identify which terms in the sum contain $\\mathbf{p}_i$:\n",
    "\n",
    "- When $i=1$: appears in term $(i=2)$: $\\|\\mathbf{p}_3 - 2\\mathbf{p}_2 + \\mathbf{p}_1\\|^2$\n",
    "- When $2 \\leq i \\leq N-2$: appears in three terms:\n",
    "  - $(i-1)$: $\\|\\mathbf{p}_i - 2\\mathbf{p}_{i-1} + \\mathbf{p}_{i-2}\\|^2$ (coefficient: $+1$)\n",
    "  - $(i)$: $\\|\\mathbf{p}_{i+1} - 2\\mathbf{p}_i + \\mathbf{p}_{i-1}\\|^2$ (coefficient: $-2$)\n",
    "  - $(i+1)$: $\\|\\mathbf{p}_{i+2} - 2\\mathbf{p}_{i+1} + \\mathbf{p}_i\\|^2$ (coefficient: $+1$)\n",
    "- When $i=N$: appears in term $(i=N-1)$: $\\|\\mathbf{p}_N - 2\\mathbf{p}_{N-1} + \\mathbf{p}_{N-2}\\|^2$\n",
    "\n",
    "Let $\\mathbf{d}_i = \\mathbf{p}_{i+1} - 2\\mathbf{p}_i + \\mathbf{p}_{i-1}$ for $i = 2, \\ldots, N-1$.\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\frac{\\partial J^{\\text{pos}}_{\\text{smoothness}}}{\\partial \\mathbf{p}_i} = \\begin{cases}\n",
    "2\\lambda_1 \\mathbf{d}_2 & i=1 \\\\\n",
    "2\\lambda_1 (\\mathbf{d}_i - 2\\mathbf{d}_{i+1} + \\mathbf{d}_{i+2}) & 2 \\leq i \\leq N-2 \\\\\n",
    "2\\lambda_1 \\mathbf{d}_{N-1} & i=N\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Or more compactly:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\frac{\\partial J^{\\text{pos}}_{\\text{smoothness}}}{\\partial \\mathbf{p}_i} = \\begin{cases}\n",
    "2\\lambda_1 (\\mathbf{p}_3 - 2\\mathbf{p}_2 + \\mathbf{p}_1) & i=1 \\\\\n",
    "2\\lambda_1 (\\mathbf{p}_{i+2} - 4\\mathbf{p}_{i+1} + 6\\mathbf{p}_i - 4\\mathbf{p}_{i-1} + \\mathbf{p}_{i-2}) & 2 \\leq i \\leq N-2 \\\\\n",
    "2\\lambda_1 (\\mathbf{p}_N - 2\\mathbf{p}_{N-1} + \\mathbf{p}_{N-2}) & i=N\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Complete Gradient with Respect to Position\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\frac{\\partial J}{\\partial \\mathbf{p}_i} = 2(\\mathbf{p}_i - \\mathbf{y}^p_i) + \\frac{\\partial J^{\\text{pos}}_{\\text{smoothness}}}{\\partial \\mathbf{p}_i}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient with Respect to Direction (Analogous)\n",
    "\n",
    "By the same derivation:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\frac{\\partial J}{\\partial \\mathbf{n}_i} = 2(\\mathbf{n}_i - \\mathbf{y}^n_i) + \\frac{\\partial J^{\\text{dir}}_{\\text{smoothness}}}{\\partial \\mathbf{n}_i}\n",
    "$\n",
    "\n",
    "where the smoothness gradient for direction follows the same structure as position.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0f68c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient shape (positions): (200, 3)\n",
      "Gradient shape (directions): (200, 3)\n",
      "\n",
      "Gradient norm (positions): 2.974671758898433\n",
      "Gradient norm (directions): 3.20997624748686\n"
     ]
    }
   ],
   "source": [
    "def gradient(p, n, y_p, y_n, lam_1, lam_2):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cost function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    p : np.array, shape (N, 3)\n",
    "        Current position estimates\n",
    "    n : np.array, shape (N, 3)\n",
    "        Current direction estimates\n",
    "    y_p : np.array, shape (N, 3)\n",
    "        Observed noisy positions\n",
    "    y_n : np.array, shape (N, 3)\n",
    "        Observed noisy directions\n",
    "    lam_1 : float\n",
    "        Position smoothness regularization parameter\n",
    "    lam_2 : float\n",
    "        Direction smoothness regularization parameter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    grad_p : np.array, shape (N, 3)\n",
    "        Gradient with respect to positions\n",
    "    grad_n : np.array, shape (N, 3)\n",
    "        Gradient with respect to directions\n",
    "    \"\"\"\n",
    "    N = p.shape[0]\n",
    "    \n",
    "    # Initialize gradients\n",
    "    grad_p = np.zeros_like(p)\n",
    "    grad_n = np.zeros_like(n)\n",
    "    \n",
    "    # ============================================\n",
    "    # Gradient of position fidelity term\n",
    "    # ============================================\n",
    "    grad_p_fidelity = 2 * (p - y_p)\n",
    "    \n",
    "    # ============================================\n",
    "    # Gradient of position smoothness term\n",
    "    # ============================================\n",
    "    grad_p_smoothness = np.zeros_like(p)\n",
    "    \n",
    "    # Boundary case: i = 0 (first point)\n",
    "    # Only appears in the term for i=1: (p[2] - 2*p[1] + p[0])\n",
    "    if N >= 3:\n",
    "        grad_p_smoothness[0] = 2 * lam_1 * (p[2] - 2*p[1] + p[0])\n",
    "    \n",
    "    # Interior points: 1 <= i <= N-2\n",
    "    # Point p[i] appears in three terms with coefficients: +1, -2, +1\n",
    "    # This gives: p[i+2] - 4*p[i+1] + 6*p[i] - 4*p[i-1] + p[i-2]\n",
    "    for i in range(1, N-1):\n",
    "        if i == 1:\n",
    "            # p[1] appears in terms i=1 (as -2) and i=2 (as +1)\n",
    "            grad_p_smoothness[i] = 2 * lam_1 * (\n",
    "                -2*(p[2] - 2*p[1] + p[0])  # derivative from term i=1\n",
    "            )\n",
    "            if N >= 4:\n",
    "                grad_p_smoothness[i] += 2 * lam_1 * (p[3] - 2*p[2] + p[1])  # derivative from term i=2\n",
    "        elif i == N-2:\n",
    "            # p[N-2] appears in terms i=N-3 (as +1) and i=N-2 (as -2)\n",
    "            grad_p_smoothness[i] = 2 * lam_1 * (\n",
    "                (p[i] - 2*p[i-1] + p[i-2])  # derivative from term i=N-3\n",
    "                - 2*(p[i+1] - 2*p[i] + p[i-1])  # derivative from term i=N-2\n",
    "            )\n",
    "        else:\n",
    "            # Interior points appear in three consecutive terms\n",
    "            grad_p_smoothness[i] = 2 * lam_1 * (\n",
    "                (p[i] - 2*p[i-1] + p[i-2])  # derivative from term (i-1)\n",
    "                - 2*(p[i+1] - 2*p[i] + p[i-1])  # derivative from term (i)\n",
    "                + (p[i+2] - 2*p[i+1] + p[i])  # derivative from term (i+1)\n",
    "            )\n",
    "    \n",
    "    # Boundary case: i = N-1 (last point)\n",
    "    if N >= 3:\n",
    "        grad_p_smoothness[N-1] = 2 * lam_1 * (p[N-1] - 2*p[N-2] + p[N-3])\n",
    "    \n",
    "    # Combine position gradients\n",
    "    grad_p = grad_p_fidelity + grad_p_smoothness\n",
    "    \n",
    "    # ============================================\n",
    "    # Gradient of direction terms (same structure)\n",
    "    # ============================================\n",
    "    grad_n_fidelity = 2 * (n - y_n)\n",
    "    \n",
    "    grad_n_smoothness = np.zeros_like(n)\n",
    "    \n",
    "    if N >= 3:\n",
    "        grad_n_smoothness[0] = 2 * lam_2 * (n[2] - 2*n[1] + n[0])\n",
    "    \n",
    "    for i in range(1, N-1):\n",
    "        if i == 1:\n",
    "            grad_n_smoothness[i] = 2 * lam_2 * (\n",
    "                -2*(n[2] - 2*n[1] + n[0])\n",
    "            )\n",
    "            if N >= 4:\n",
    "                grad_n_smoothness[i] += 2 * lam_2 * (n[3] - 2*n[2] + n[1])\n",
    "        elif i == N-2:\n",
    "            grad_n_smoothness[i] = 2 * lam_2 * (\n",
    "                (n[i] - 2*n[i-1] + n[i-2])\n",
    "                - 2*(n[i+1] - 2*n[i] + n[i-1])\n",
    "            )\n",
    "        else:\n",
    "            grad_n_smoothness[i] = 2 * lam_2 * (\n",
    "                (n[i] - 2*n[i-1] + n[i-2])\n",
    "                - 2*(n[i+1] - 2*n[i] + n[i-1])\n",
    "                + (n[i+2] - 2*n[i+1] + n[i])\n",
    "            )\n",
    "    \n",
    "    if N >= 3:\n",
    "        grad_n_smoothness[N-1] = 2 * lam_2 * (n[N-1] - 2*n[N-2] + n[N-3])\n",
    "    \n",
    "    # Combine direction gradients\n",
    "    grad_n = grad_n_fidelity + grad_n_smoothness\n",
    "    \n",
    "    return grad_p, grad_n\n",
    "\n",
    "\n",
    "# Test the gradient function\n",
    "data = np.load(\"drone_trajectory.npz\")\n",
    "trajectory = data['trajectory']\n",
    "trajectory_noisy = data['trajectory_noisy']\n",
    "\n",
    "y_p = trajectory_noisy[:, :3]  # noisy positions\n",
    "y_n = trajectory_noisy[:, 3:]  # noisy directions\n",
    "\n",
    "p = y_p.copy()  # initial guess\n",
    "n = y_n.copy()\n",
    "\n",
    "lam_1 = 0.1\n",
    "lam_2 = 0.1\n",
    "\n",
    "grad_p, grad_n = gradient(p, n, y_p, y_n, lam_1, lam_2)\n",
    "\n",
    "print(\"Gradient shape (positions):\", grad_p.shape)\n",
    "print(\"Gradient shape (directions):\", grad_n.shape)\n",
    "print(\"\\nGradient norm (positions):\", np.linalg.norm(grad_p))\n",
    "print(\"Gradient norm (directions):\", np.linalg.norm(grad_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876f81a",
   "metadata": {},
   "source": [
    "## Line Search Methods\n",
    "\n",
    "Line search methods are iterative optimization algorithms that follow this general pattern:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha \\mathbf{d}_k\n",
    "$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{x}_k$ is the current iterate\n",
    "- $\\alpha > 0$ is a **constant step size** (learning rate)\n",
    "- $\\mathbf{d}_k$ is the search direction\n",
    "\n",
    "---\n",
    "\n",
    "### First-Order Method: Gradient Descent\n",
    "\n",
    "The search direction is the **negative gradient**:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\mathbf{d}_k = -\\nabla J(\\mathbf{x}_k)\n",
    "$\n",
    "\n",
    "This direction guarantees descent because:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\nabla J(\\mathbf{x}_k)^T \\mathbf{d}_k = -\\|\\nabla J(\\mathbf{x}_k)\\|^2 < 0\n",
    "$\n",
    "\n",
    "**Update rule:**\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\nabla J(\\mathbf{x}_k)\n",
    "$\n",
    "\n",
    "The step size $\\alpha$ must be chosen carefully:\n",
    "- Too large: overshooting, divergence\n",
    "- Too small: slow convergence\n",
    "\n",
    "---\n",
    "\n",
    "### Second-Order Method: Newton's Method\n",
    "\n",
    "The search direction uses **second-order information** (Hessian):\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\mathbf{d}_k = -\\mathbf{H}_k^{-1} \\nabla J(\\mathbf{x}_k)\n",
    "$\n",
    "\n",
    "where $\\mathbf{H}_k = \\nabla^2 J(\\mathbf{x}_k)$ is the Hessian matrix.\n",
    "\n",
    "**Update rule:**\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\mathbf{H}_k^{-1} \\nabla J(\\mathbf{x}_k)\n",
    "$\n",
    "\n",
    "Newton's method has **quadratic convergence** near the minimum when $\\mathbf{H}_k$ is positive definite.\n",
    "\n",
    "For our problem, we typically use $\\alpha = 1.0$ for Newton's method since the Hessian already provides optimal scaling.\n",
    "\n",
    "---\n",
    "## Hessian Matrix of the Cost Function\n",
    "\n",
    "The Hessian matrix contains all second-order partial derivatives:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\mathbf{H} = \\nabla^2 J(\\mathbf{p}, \\mathbf{n}) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 J}{\\partial \\mathbf{p} \\partial \\mathbf{p}} & \\frac{\\partial^2 J}{\\partial \\mathbf{p} \\partial \\mathbf{n}} \\\\\n",
    "\\frac{\\partial^2 J}{\\partial \\mathbf{n} \\partial \\mathbf{p}} & \\frac{\\partial^2 J}{\\partial \\mathbf{n} \\partial \\mathbf{n}}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Since $J(\\mathbf{p}, \\mathbf{n})$ separates into independent position and direction terms:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\mathbf{H} = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{H}_{\\mathbf{p}} & \\mathbf{0} \\\\\n",
    "\\mathbf{0} & \\mathbf{H}_{\\mathbf{n}}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "where $\\frac{\\partial^2 J}{\\partial \\mathbf{p} \\partial \\mathbf{n}} = \\mathbf{0}$ (positions and directions are decoupled).\n",
    "\n",
    "---\n",
    "\n",
    "### Hessian of Position Terms\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\mathbf{H}_{\\mathbf{p}} = \\frac{\\partial^2 J}{\\partial \\mathbf{p}^2} = \\frac{\\partial^2 J^{\\text{pos}}_{\\text{fidelity}}}{\\partial \\mathbf{p}^2} + \\frac{\\partial^2 J^{\\text{pos}}_{\\text{smoothness}}}{\\partial \\mathbf{p}^2}\n",
    "$\n",
    "\n",
    "**Hessian of Fidelity Term:**\n",
    "\n",
    "$\n",
    "\\Large\n",
    "J^{\\text{pos}}_{\\text{fidelity}} = \\sum_{i=1}^{N} \\|\\mathbf{p}_i - \\mathbf{y}^p_i\\|^2\n",
    "$\n",
    "\n",
    "The second derivative gives:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\frac{\\partial^2 J^{\\text{pos}}_{\\text{fidelity}}}{\\partial \\mathbf{p}_i \\partial \\mathbf{p}_j} = \\begin{cases}\n",
    "2\\mathbf{I}_{3 \\times 3} & i = j \\\\\n",
    "\\mathbf{0}_{3 \\times 3} & i \\neq j\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "This contributes $2\\mathbf{I}$ to the diagonal blocks.\n",
    "\n",
    "---\n",
    "\n",
    "**Hessian of Smoothness Term:**\n",
    "\n",
    "$\n",
    "\\Large\n",
    "J^{\\text{pos}}_{\\text{smoothness}} = \\lambda_1 \\sum_{i=2}^{N-1} \\|\\mathbf{p}_{i+1} - 2\\mathbf{p}_i + \\mathbf{p}_{i-1}\\|^2\n",
    "$\n",
    "\n",
    "For a single term $\\|\\mathbf{p}_{i+1} - 2\\mathbf{p}_i + \\mathbf{p}_{i-1}\\|^2$, the Hessian has a **pentadiagonal structure**.\n",
    "\n",
    "Let's compute: $\\mathbf{d}_i = \\mathbf{p}_{i+1} - 2\\mathbf{p}_i + \\mathbf{p}_{i-1}$\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\|\\mathbf{d}_i\\|^2 = \\mathbf{d}_i^T \\mathbf{d}_i\n",
    "$\n",
    "\n",
    "The second derivatives are:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\frac{\\partial^2 \\|\\mathbf{d}_i\\|^2}{\\partial \\mathbf{p}_j \\partial \\mathbf{p}_k} = \n",
    "\\begin{cases}\n",
    "2 \\cdot 1 \\cdot \\mathbf{I} = 2\\mathbf{I} & (j,k) = (i-1, i-1) \\text{ or } (i+1, i+1) \\\\\n",
    "2 \\cdot 4 \\cdot \\mathbf{I} = 8\\mathbf{I} & (j,k) = (i, i) \\\\\n",
    "2 \\cdot (-2) \\cdot \\mathbf{I} = -4\\mathbf{I} & (j,k) \\in \\{(i-1,i), (i,i-1), (i,i+1), (i+1,i)\\} \\\\\n",
    "2 \\cdot 1 \\cdot \\mathbf{I} = 2\\mathbf{I} & (j,k) \\in \\{(i-1,i+1), (i+1,i-1)\\} \\\\\n",
    "\\mathbf{0} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "**Structure of $\\mathbf{H}_{\\mathbf{p}}$:**\n",
    "\n",
    "The Hessian is a **block pentadiagonal matrix** where each block is $3 \\times 3$:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\mathbf{H}_{\\mathbf{p}} = 2\\mathbf{I} + 2\\lambda_1\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{A}_1 & \\mathbf{B}_1 & \\mathbf{C}_1 & & & \\\\\n",
    "\\mathbf{B}_1^T & \\mathbf{A}_2 & \\mathbf{B}_2 & \\mathbf{C}_2 & & \\\\\n",
    "\\mathbf{C}_1^T & \\mathbf{B}_2^T & \\mathbf{A}_3 & \\mathbf{B}_3 & \\ddots & \\\\\n",
    "& \\mathbf{C}_2^T & \\ddots & \\ddots & \\ddots & \\mathbf{C}_{N-3} \\\\\n",
    "& & \\ddots & \\mathbf{B}_{N-2}^T & \\mathbf{A}_{N-1} & \\mathbf{B}_{N-1} \\\\\n",
    "& & & \\mathbf{C}_{N-3}^T & \\mathbf{B}_{N-1}^T & \\mathbf{A}_N\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "where the diagonal blocks are:\n",
    "- $\\mathbf{A}_1 = \\mathbf{I}$ (boundary)\n",
    "- $\\mathbf{A}_i = 6\\mathbf{I}$ for $2 \\leq i \\leq N-1$ (interior)\n",
    "- $\\mathbf{A}_N = \\mathbf{I}$ (boundary)\n",
    "\n",
    "and off-diagonal blocks:\n",
    "- $\\mathbf{B}_i = -4\\mathbf{I}$ (adjacent interactions)\n",
    "- $\\mathbf{C}_i = \\mathbf{I}$ (two-step interactions)\n",
    "\n",
    "---\n",
    "\n",
    "### Hessian of Direction Terms\n",
    "\n",
    "By symmetry:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\mathbf{H}_{\\mathbf{n}} = 2\\mathbf{I} + 2\\lambda_2 \\cdot [\\text{same pentadiagonal structure}]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Properties of the Hessian\n",
    "\n",
    "1. **Symmetric**: $\\mathbf{H}^T = \\mathbf{H}$\n",
    "2. **Block-diagonal**: Position and direction terms are decoupled\n",
    "3. **Sparse**: Pentadiagonal structure (only 5 diagonals have non-zero blocks)\n",
    "4. **Positive definite** when $\\lambda_1, \\lambda_2 > 0$: guarantees a unique global minimum\n",
    "\n",
    "</div>"
   ]
  },
  
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eee507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
